{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b73d3e",
   "metadata": {},
   "source": [
    "### 2. Understanding Partitions\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "**1.**  Understand how Spark partitions the data.\n",
    "\n",
    "**2.** Interact with the partitions of a dataframe directly.\n",
    "\n",
    "**3.** See the step in the physical plan that corresponds to shuffle.\n",
    "\n",
    "Spark works with large data by separating the data into several partitions in manageable sizes. Then, the executors in the cluster can work with the partitions. I like to think of each partition as a local dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9869cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/root/miniconda/envs/py36/lib/python3.6/site-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.sql.shuffle.partitions\", 16).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df5a716",
   "metadata": {},
   "source": [
    "Following cell creates the necessary data. You don't have to understand the code. You just need to know that:\n",
    "\n",
    "- This is typical dataset (``daily_data``) that we deal with every day.\n",
    "\n",
    "- Resulting dataframe contains ``n_stores`` x ``n_products`` pairs.\n",
    "\n",
    "- You can specify start and end dates.\n",
    "\n",
    "- Resulting dataframe is partitioned by data and written in parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93dd4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_demo_data(n_products, n_stores, start_date=\"2021-01-01\", end_date=\"2022-01-01\"):\n",
    "    \"\"\"Creates demo data, writes it as parquet partitioned by date, reads it and returns the dataframe\"\"\"\n",
    "    dates = pd.date_range(start_date, end_date)\n",
    "    dates = [str(date)[:10] for date in dates]\n",
    "\n",
    "    day_index = np.arange(len(dates))\n",
    "    result = []\n",
    "    for product in range(n_products):\n",
    "        for store in range(n_stores):\n",
    "            sales = np.random.poisson(10, size=len(dates))\n",
    "            partial_df = (\n",
    "                pd.DataFrame(dates, columns=[\"date\"])\n",
    "                .assign(product_id=product)\n",
    "                .assign(store_id=store)\n",
    "                .assign(day_index=day_index)\n",
    "                .assign(sales_quantity=sales)\n",
    "            )\n",
    "            result.append(partial_df)\n",
    "    pdf = pd.concat(result)\n",
    "    result = spark.createDataFrame(pdf)\n",
    "    result.repartition(\"date\").write.partitionBy(\"date\").parquet(\"demo-data\", mode=\"overwrite\")\n",
    "    return spark.read.parquet(\"demo-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c7aa2",
   "metadata": {},
   "source": [
    "Let's create a dataset that only contains a single pair and 31 days of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45442b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_demo_data(n_products=1, n_stores=1, start_date=\"2021-01-01\", end_date=\"2021-01-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "305b5003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195c73a",
   "metadata": {},
   "source": [
    "We can obtain the number of partitions in the dataframe by ``rdd.getNumPartitions``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4733b3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba9c9ec",
   "metadata": {},
   "source": [
    "Although we have 31 dates, the dataframe has 16 partitions (decided by ``spark.sql.shuffle.partitions``).\n",
    "\n",
    "We can interact with partitions directly using ``rdd.glom`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c61ed10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method glom in module pyspark.rdd:\n",
      "\n",
      "glom() method of pyspark.rdd.RDD instance\n",
      "    Return an RDD created by coalescing all elements within each partition\n",
      "    into a list.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      "    >>> sorted(rdd.glom().collect())\n",
      "    [[1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.rdd.glom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d53ea4",
   "metadata": {},
   "source": [
    "Let's count the number of rows in each partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "920f8272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.glom().map(lambda x: len(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6587aa3",
   "metadata": {},
   "source": [
    "Spark tries to divide the partitions evenly as much as it's possible. Here, we have 2 rows of data at each partition, except for the last partition. Last partition only contains a single row. We can also collect the data in each partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52da9418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(product_id=0, store_id=0, day_index=17, sales_quantity=8, date=datetime.date(2021, 1, 18)),\n",
       "  Row(product_id=0, store_id=0, day_index=9, sales_quantity=12, date=datetime.date(2021, 1, 10))],\n",
       " [Row(product_id=0, store_id=0, day_index=6, sales_quantity=8, date=datetime.date(2021, 1, 7)),\n",
       "  Row(product_id=0, store_id=0, day_index=8, sales_quantity=11, date=datetime.date(2021, 1, 9))],\n",
       " [Row(product_id=0, store_id=0, day_index=5, sales_quantity=6, date=datetime.date(2021, 1, 6)),\n",
       "  Row(product_id=0, store_id=0, day_index=27, sales_quantity=15, date=datetime.date(2021, 1, 28))],\n",
       " [Row(product_id=0, store_id=0, day_index=21, sales_quantity=7, date=datetime.date(2021, 1, 22)),\n",
       "  Row(product_id=0, store_id=0, day_index=16, sales_quantity=14, date=datetime.date(2021, 1, 17))],\n",
       " [Row(product_id=0, store_id=0, day_index=4, sales_quantity=14, date=datetime.date(2021, 1, 5)),\n",
       "  Row(product_id=0, store_id=0, day_index=11, sales_quantity=12, date=datetime.date(2021, 1, 12))],\n",
       " [Row(product_id=0, store_id=0, day_index=0, sales_quantity=9, date=datetime.date(2021, 1, 1)),\n",
       "  Row(product_id=0, store_id=0, day_index=29, sales_quantity=11, date=datetime.date(2021, 1, 30))],\n",
       " [Row(product_id=0, store_id=0, day_index=26, sales_quantity=3, date=datetime.date(2021, 1, 27)),\n",
       "  Row(product_id=0, store_id=0, day_index=22, sales_quantity=9, date=datetime.date(2021, 1, 23))],\n",
       " [Row(product_id=0, store_id=0, day_index=15, sales_quantity=13, date=datetime.date(2021, 1, 16)),\n",
       "  Row(product_id=0, store_id=0, day_index=14, sales_quantity=3, date=datetime.date(2021, 1, 15))],\n",
       " [Row(product_id=0, store_id=0, day_index=1, sales_quantity=13, date=datetime.date(2021, 1, 2)),\n",
       "  Row(product_id=0, store_id=0, day_index=2, sales_quantity=9, date=datetime.date(2021, 1, 3))],\n",
       " [Row(product_id=0, store_id=0, day_index=18, sales_quantity=6, date=datetime.date(2021, 1, 19)),\n",
       "  Row(product_id=0, store_id=0, day_index=19, sales_quantity=12, date=datetime.date(2021, 1, 20))],\n",
       " [Row(product_id=0, store_id=0, day_index=13, sales_quantity=14, date=datetime.date(2021, 1, 14)),\n",
       "  Row(product_id=0, store_id=0, day_index=24, sales_quantity=12, date=datetime.date(2021, 1, 25))],\n",
       " [Row(product_id=0, store_id=0, day_index=12, sales_quantity=11, date=datetime.date(2021, 1, 13)),\n",
       "  Row(product_id=0, store_id=0, day_index=7, sales_quantity=16, date=datetime.date(2021, 1, 8))],\n",
       " [Row(product_id=0, store_id=0, day_index=23, sales_quantity=9, date=datetime.date(2021, 1, 24)),\n",
       "  Row(product_id=0, store_id=0, day_index=28, sales_quantity=10, date=datetime.date(2021, 1, 29))],\n",
       " [Row(product_id=0, store_id=0, day_index=20, sales_quantity=10, date=datetime.date(2021, 1, 21)),\n",
       "  Row(product_id=0, store_id=0, day_index=10, sales_quantity=9, date=datetime.date(2021, 1, 11))],\n",
       " [Row(product_id=0, store_id=0, day_index=25, sales_quantity=10, date=datetime.date(2021, 1, 26)),\n",
       "  Row(product_id=0, store_id=0, day_index=3, sales_quantity=10, date=datetime.date(2021, 1, 4))],\n",
       " [Row(product_id=0, store_id=0, day_index=30, sales_quantity=16, date=datetime.date(2021, 1, 31))]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11aa2d",
   "metadata": {},
   "source": [
    "Now we will create a dataset that has two products, one store and five days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d48d4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_demo_data(n_products=2, n_stores=1, start_date=\"2021-01-01\", end_date=\"2021-01-05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441018b2",
   "metadata": {},
   "source": [
    "See that we have 5 partitions (one for each date) now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38fbbbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ad1dcf",
   "metadata": {},
   "source": [
    "The parquet file format contains the partition information, and we have written the dataframe partitioned by date. Thus, we see that all rows for a date are in the same partition. For example, the first partition contains all rows for dates ``'2021-01-05'`` and ``'2021-01-04'``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2868fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(product_id=0, store_id=0, day_index=4, sales_quantity=9, date=datetime.date(2021, 1, 5)),\n",
       "  Row(product_id=1, store_id=0, day_index=4, sales_quantity=8, date=datetime.date(2021, 1, 5))],\n",
       " [Row(product_id=0, store_id=0, day_index=3, sales_quantity=8, date=datetime.date(2021, 1, 4)),\n",
       "  Row(product_id=1, store_id=0, day_index=3, sales_quantity=14, date=datetime.date(2021, 1, 4))],\n",
       " [Row(product_id=0, store_id=0, day_index=1, sales_quantity=7, date=datetime.date(2021, 1, 2)),\n",
       "  Row(product_id=1, store_id=0, day_index=1, sales_quantity=15, date=datetime.date(2021, 1, 2))],\n",
       " [Row(product_id=0, store_id=0, day_index=0, sales_quantity=16, date=datetime.date(2021, 1, 1)),\n",
       "  Row(product_id=1, store_id=0, day_index=0, sales_quantity=9, date=datetime.date(2021, 1, 1))],\n",
       " [Row(product_id=0, store_id=0, day_index=2, sales_quantity=9, date=datetime.date(2021, 1, 3)),\n",
       "  Row(product_id=1, store_id=0, day_index=2, sales_quantity=7, date=datetime.date(2021, 1, 3))]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541dde33",
   "metadata": {},
   "source": [
    "Repartition by ``product_id`` and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86a34307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(\"product_id\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76583a9e",
   "metadata": {},
   "source": [
    "In this case there is a single partition. I find it easy to think that there are 16 partitions, but 15 of them are empty.\n",
    "\n",
    "**Takeaways:**\n",
    "\n",
    "- Just because there is two products does not mean there will be two partitions. It means that all rows for a product will be in the same partition. In this case, we have a single partition.\n",
    "\n",
    "\n",
    "Docstring for HashPartitioning in Spark: [partitioning.scala](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/physical/partitioning.scala)\n",
    "\n",
    "\n",
    "```scala\n",
    "/**\n",
    " * Represents a partitioning where rows are split up across partitions based on the hash\n",
    " * of `expressions`.  All rows where `expressions` evaluate to the same values are guaranteed to be\n",
    " * in the same partition.\n",
    " */\n",
    " case class HashPartitioning(expressions: Seq[Expression], numPartitions: Int)\n",
    " ```\n",
    " \n",
    " It also means that we can pass expressions to ``repartition`` method: For example: ``df.repartition(F.ceil(F.rand() * 100))`` is valid.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ebfea6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(F.col(\"sales_quantity\") > 5).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de5c9d",
   "metadata": {},
   "source": [
    "**Shuffle in physical plan**\n",
    "\n",
    "``Exchange`` step in physical plan corresponds to shuffling of data between executors. It is an expensive operation since all the data will move between executors across the cluster. This is also a typical cause of disk spill since the executors might need to write data to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1b12cb0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange hashpartitioning(product_id#52L, 16), REPARTITION_BY_COL, [id=#165]\n",
      "   +- FileScan parquet [product_id#52L,store_id#53L,day_index#54L,sales_quantity#55L,date#56] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/rocks/Untitled Folder/demo-data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:bigint,store_id:bigint,day_index:bigint,sales_quantity:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.repartition(\"product_id\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e84023c",
   "metadata": {},
   "source": [
    "Here, we did not specify the number of partitions but the ``hashpartitioning`` step shows that we requested 16 partitions (decided by ``spark.sql.shuffle.partitions``). This is why tuning ``spark.sql.shuffle.partitions`` is so important. Whenever there is a shuffle, resulting number of partitions depend on ``spark.sql.shuffle.partitions.`` See:\n",
    "\n",
    "+- Exchange hashpartitioning(product_id#206L, **16**), REPARTITION_BY_COL, [id=#480]\n",
    "\n",
    "Shuffle often referred as a necessary evil, but it turns out we can sometimes reduce the number of shuffles necessary (which is the topic of another notebook). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c28b7",
   "metadata": {},
   "source": [
    "**Bonus:** Low level ``rdd`` API. \n",
    "\n",
    "- ``rdd`` API can be used to perform custom low level operations. Using ``DataFrame`` API is suggested over low level ``rdd`` API since ``DataFrame`` API provides several optimizations. Still, they are useful to know about. Also, there might still be some use cases.\n",
    "\n",
    "With ``rdd.mapPartitions`` we can perform a custom mapping to each partition. Each partition should enough information to compute its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a1bf263",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method mapPartitions in module pyspark.rdd:\n",
      "\n",
      "mapPartitions(f, preservesPartitioning=False) method of pyspark.rdd.RDD instance\n",
      "    Return a new RDD by applying a function to each partition of this RDD.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      "    >>> def f(iterator): yield sum(iterator)\n",
      "    >>> rdd.mapPartitions(f).collect()\n",
      "    [3, 7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.rdd.mapPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd385761",
   "metadata": {},
   "source": [
    "**Exercise**: Find the maximum sales of each partition with ``rdd.mapPartitions``. Remember we have 5 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b03cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sales(partition):\n",
    "    max_sales = 0\n",
    "    for row in partition:\n",
    "        sales = row.sales_quantity or 0\n",
    "        if sales > max_sales:\n",
    "            max_sales = sales\n",
    "    yield (max_sales,) # needs to be tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f2414d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|max_sales|\n",
      "+---------+\n",
      "|        9|\n",
      "|       14|\n",
      "|       15|\n",
      "|       16|\n",
      "|        9|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.rdd.mapPartitions(max_sales).toDF([\"max_sales\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe4b45",
   "metadata": {},
   "source": [
    "``rdd.mapPartitions`` is very flexible. The return value can be anything, it just applies a function to a partition.\n",
    "\n",
    "Here is a good example. We can fit a ``LinearRegression`` to each partition and return the fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5ea0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def fit_linear_regression(partition):\n",
    "    pdf = pd.DataFrame(partition, columns=[\"product_id\", \"store_id\", \"day_index\", \"sales_quantity\", \"date\"])\n",
    "    lr = LinearRegression().fit(pdf.loc[:, [\"product_id\", \"day_index\"]], pdf.sales_quantity)\n",
    "    yield (lr,) # needs to be tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d69b3152",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.rdd.mapPartitions(fit_linear_regression).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15cd918c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(LinearRegression(),),\n",
       " (LinearRegression(),),\n",
       " (LinearRegression(),),\n",
       " (LinearRegression(),),\n",
       " (LinearRegression(),)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bfb12cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0].coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
