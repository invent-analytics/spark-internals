{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efaa3599",
   "metadata": {},
   "source": [
    "In this notebook, we will analyze the physical plans of some basic window calculations and demonstrate two optimization possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4cd7fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/root/miniconda/envs/py36/lib/python3.6/site-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.sql.shuffle.partitions\", 16).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590d8ff",
   "metadata": {},
   "source": [
    "Following cell creates the demo dataset. It is the same function used in the first notebook. You can skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0bbdbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_demo_data(n_products, n_stores, start_date=\"2021-01-01\", end_date=\"2022-01-01\"):\n",
    "    \"\"\"Creates demo data, writes it as parquet partitioned by date, reads it and returns the dataframe\"\"\"\n",
    "    dates = pd.date_range(start_date, end_date)\n",
    "    dates = [str(date)[:10] for date in dates]\n",
    "\n",
    "    day_index = np.arange(len(dates))\n",
    "    result = []\n",
    "    for product in range(n_products):\n",
    "        for store in range(n_stores):\n",
    "            sales = np.random.poisson(10, size=len(dates))\n",
    "            partial_df = (\n",
    "                pd.DataFrame(dates, columns=[\"date\"])\n",
    "                .assign(product_id=product)\n",
    "                .assign(store_id=store)\n",
    "                .assign(day_index=day_index)\n",
    "                .assign(sales_quantity=sales)\n",
    "                .assign(product_agg_level=\"wine\")\n",
    "            )\n",
    "            result.append(partial_df)\n",
    "    pdf = pd.concat(result)\n",
    "    result = spark.createDataFrame(pdf)\n",
    "    result.repartition(\"date\").write.partitionBy(\"date\").parquet(\"demo-data\", mode=\"overwrite\")\n",
    "    return spark.read.parquet(\"demo-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e2e926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_demo_data(n_products=10, n_stores=10, start_date=\"2021-01-01\", end_date=\"2021-01-31\")\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32b2a2",
   "metadata": {},
   "source": [
    "### A simple rolling sum\n",
    "\n",
    "Calculate moving total sales for each product/store pair for last 4 days, including current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a0f686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Window [sum(sales_quantity#22L) windowspecdefinition(store_id#20L, product_id#19L, day_index#21L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -3, currentrow$())) AS moving_sum_sales#32L], [store_id#20L, product_id#19L], [day_index#21L ASC NULLS FIRST]\n",
      "   +- Sort [store_id#20L ASC NULLS FIRST, product_id#19L ASC NULLS FIRST, day_index#21L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(store_id#20L, product_id#19L, 16), ENSURE_REQUIREMENTS, [id=#41]\n",
      "         +- FileScan parquet [product_id#19L,store_id#20L,day_index#21L,sales_quantity#22L,product_agg_level#23,date#24] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/rocks/Untitled Folder/demo-data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:bigint,store_id:bigint,day_index:bigint,sales_quantity:bigint,product_agg_level...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"store_id\", \"product_id\").orderBy(\"day_index\").rangeBetween(-3, 0)\n",
    "\n",
    "result = df.withColumn(\"moving_sum_sales\", F.sum(\"sales_quantity\").over(w))\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2699c",
   "metadata": {},
   "source": [
    "What is interesting with this physical plan is there is a ``Exchange hashpartitioning`` step that repartitions the data by ``product_id``/``store_id``. Spark repartitions your data according to the columns passed to ``partitionBy``. The ``rangeBetween`` or ``rowsBetween`` expression do not change the shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d89c8",
   "metadata": {},
   "source": [
    "### Multiple uses of the same window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4239d8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Window [sum(sales_quantity#22L) windowspecdefinition(store_id#20L, product_id#19L, day_index#21L ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, 1)) AS moving_sum_sales#41L, max(date#24) windowspecdefinition(store_id#20L, product_id#19L, day_index#21L ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, 1)) AS max_date#50, avg(sales_quantity#22L) windowspecdefinition(store_id#20L, product_id#19L, day_index#21L ASC NULLS FIRST, specifiedwindowframe(RowFrame, -10, currentrow$())) AS mean_sales#60], [store_id#20L, product_id#19L], [day_index#21L ASC NULLS FIRST]\n",
      "   +- Sort [store_id#20L ASC NULLS FIRST, product_id#19L ASC NULLS FIRST, day_index#21L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(store_id#20L, product_id#19L, 16), ENSURE_REQUIREMENTS, [id=#52]\n",
      "         +- FileScan parquet [product_id#19L,store_id#20L,day_index#21L,sales_quantity#22L,product_agg_level#23,date#24] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/rocks/Untitled Folder/demo-data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:bigint,store_id:bigint,day_index:bigint,sales_quantity:bigint,product_agg_level...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1 = Window.partitionBy(\"store_id\", \"product_id\").orderBy(\"day_index\").rowsBetween(-1, 1)\n",
    "w2 = Window.partitionBy(\"store_id\", \"product_id\").orderBy(\"day_index\").rowsBetween(-10, 0)\n",
    "\n",
    "result = (\n",
    "     df\n",
    "    .withColumn(\"moving_sum_sales\", F.sum(\"sales_quantity\").over(w1))\n",
    "    .withColumn(\"max_date\", F.max(\"date\").over(w1))\n",
    "    .withColumn(\"mean_sales\", F.mean(\"sales_quantity\").over(w2))\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92771803",
   "metadata": {},
   "source": [
    "**Takeaway:**\n",
    "\n",
    "- Spark avoids additional shuffles if multiple windows use the same ``partitionBy``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94fbb90",
   "metadata": {},
   "source": [
    "### Windows using different ``partitionBy``\n",
    "\n",
    "It turns out in some cases, we can also avoid unnecessary shuffles if we re-organize our window expressions.\n",
    "\n",
    "**Two windows, two shuffles**\n",
    "\n",
    "1st: partitioned by ``product_id``, ``store_id``\n",
    "\n",
    "2nd: partitioned by ``store_id``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00888515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Window [avg(sales_quantity#22L) windowspecdefinition(store_id#20L, day_index#21L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -3, currentrow$())) AS store_mean_-3_0#80], [store_id#20L], [day_index#21L ASC NULLS FIRST]\n",
      "   +- Sort [store_id#20L ASC NULLS FIRST, day_index#21L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(store_id#20L, 16), ENSURE_REQUIREMENTS, [id=#71]\n",
      "         +- Window [avg(sales_quantity#22L) windowspecdefinition(product_id#19L, store_id#20L, day_index#21L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -3, currentrow$())) AS pair_mean_-3_0#71], [product_id#19L, store_id#20L], [day_index#21L ASC NULLS FIRST]\n",
      "            +- Sort [product_id#19L ASC NULLS FIRST, store_id#20L ASC NULLS FIRST, day_index#21L ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(product_id#19L, store_id#20L, 16), ENSURE_REQUIREMENTS, [id=#67]\n",
      "                  +- FileScan parquet [product_id#19L,store_id#20L,day_index#21L,sales_quantity#22L,product_agg_level#23,date#24] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/rocks/Untitled Folder/demo-data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:bigint,store_id:bigint,day_index:bigint,sales_quantity:bigint,product_agg_level...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1 = Window.partitionBy(\"product_id\", \"store_id\").orderBy(\"day_index\").rangeBetween(-3, 0)\n",
    "w2 = Window.partitionBy(\"store_id\").orderBy(\"day_index\").rangeBetween(-3, 0)\n",
    "\n",
    "res = (\n",
    "    df\n",
    "    .withColumn(\"pair_mean_-3_0\", F.mean(\"sales_quantity\").over(w1))\n",
    "    .withColumn(\"store_mean_-3_0\", F.mean(\"sales_quantity\").over(w2))\n",
    "\n",
    ")\n",
    "res.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c6fc0",
   "metadata": {},
   "source": [
    "See that we have now two ``Exchange hashpartitioning`` steps:\n",
    "\n",
    "**1:** ``+- Exchange hashpartitioning(product_id#42L, store_id#43L, 16), ENSURE_REQUIREMENTS, [id=#171]``\n",
    "\n",
    "**2:**: ``+- Exchange hashpartitioning(store_id#43L, 16), ENSURE_REQUIREMENTS, [id=#175]\n",
    "``\n",
    "\n",
    "But why? If we shuffled the data by ``store_id`` first, we wouldn't need to shuffle again since all rows for a ``store_id`` would be in the same partition. We can optimize just by reordering the ``partitionBy`` columns in the first window:\n",
    "\n",
    "\n",
    "**Two windows re-organized, single shuffle**\n",
    "\n",
    "1st: partitioned by ``store_id``, ``product_id``\n",
    "\n",
    "2nd: partitioned by ``store_id``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b807c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [product_id#19L, store_id#20L, day_index#21L, sales_quantity#22L, product_agg_level#23, date#24, pair_mean_-3_0#90, store_mean_-3_0#99]\n",
      "   +- Window [avg(sales_quantity#22L) windowspecdefinition(store_id#20L, product_id#19L, day_index#21L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -3, currentrow$())) AS pair_mean_-3_0#90], [store_id#20L, product_id#19L], [day_index#21L ASC NULLS FIRST]\n",
      "      +- Sort [store_id#20L ASC NULLS FIRST, product_id#19L ASC NULLS FIRST, day_index#21L ASC NULLS FIRST], false, 0\n",
      "         +- Window [avg(sales_quantity#22L) windowspecdefinition(store_id#20L, day_index#21L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -3, currentrow$())) AS store_mean_-3_0#99], [store_id#20L], [day_index#21L ASC NULLS FIRST]\n",
      "            +- Sort [store_id#20L ASC NULLS FIRST, day_index#21L ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(store_id#20L, 16), ENSURE_REQUIREMENTS, [id=#90]\n",
      "                  +- FileScan parquet [product_id#19L,store_id#20L,day_index#21L,sales_quantity#22L,product_agg_level#23,date#24] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/rocks/Untitled Folder/demo-data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:bigint,store_id:bigint,day_index:bigint,sales_quantity:bigint,product_agg_level...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1 = Window.partitionBy(\"store_id\", \"product_id\").orderBy(\"day_index\").rangeBetween(-3, 0)\n",
    "w2 = Window.partitionBy(\"store_id\").orderBy(\"day_index\").rangeBetween(-3, 0)\n",
    "\n",
    "res = (\n",
    "    df\n",
    "    .withColumn(\"pair_mean_-3_0\", F.mean(\"sales_quantity\").over(w1))\n",
    "    .withColumn(\"store_mean_-3_0\", F.mean(\"sales_quantity\").over(w2))\n",
    "\n",
    ")\n",
    "res.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe4851",
   "metadata": {},
   "source": [
    "Now we have a single ``Exchange hashpartitioning`` step and we can avoid the second shuffle. Much better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd63029",
   "metadata": {},
   "source": [
    "### Employing our business logic in window expressions\n",
    "\n",
    "In a typical situation we might want to perform window calculations over different aggregation levels. We know that the granularity (from granular to coarse) is as follows:\n",
    "\n",
    "``product_id`` -> ``product_agg_level_5`` -> ``product_agg_level_4`` etc.\n",
    "\n",
    "But spark has no idea. By writing our window specifications explicitly we can avoid more shuffles:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f70e38",
   "metadata": {},
   "source": [
    "**Naive approach results in two shuffles**:\n",
    "\n",
    "1st window partitioned by: ``product_id``, ``store_id``\n",
    "\n",
    "2nd window partitioned by: ``product_agg_level``, ``store_id``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "955a8f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Window [avg(sales_quantity#22L) windowspecdefinition(product_agg_level#23, store_id#20L, day_index#21L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -5, 5)) AS agg_level_store_mean#118], [product_agg_level#23, store_id#20L], [day_index#21L ASC NULLS FIRST]\n",
      "   +- Sort [product_agg_level#23 ASC NULLS FIRST, store_id#20L ASC NULLS FIRST, day_index#21L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(product_agg_level#23, store_id#20L, 16), ENSURE_REQUIREMENTS, [id=#113]\n",
      "         +- Window [avg(sales_quantity#22L) windowspecdefinition(store_id#20L, product_id#19L, day_index#21L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -3, currentrow$())) AS store_product_mean#109], [store_id#20L, product_id#19L], [day_index#21L ASC NULLS FIRST]\n",
      "            +- Sort [store_id#20L ASC NULLS FIRST, product_id#19L ASC NULLS FIRST, day_index#21L ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(store_id#20L, product_id#19L, 16), ENSURE_REQUIREMENTS, [id=#109]\n",
      "                  +- FileScan parquet [product_id#19L,store_id#20L,day_index#21L,sales_quantity#22L,product_agg_level#23,date#24] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/rocks/Untitled Folder/demo-data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:bigint,store_id:bigint,day_index:bigint,sales_quantity:bigint,product_agg_level...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "w1 = Window.partitionBy(\"store_id\", \"product_id\").orderBy(\"day_index\").rangeBetween(-3, 0)\n",
    "w2 = Window.partitionBy(\"product_agg_level\", \"store_id\").orderBy(\"day_index\").rangeBetween(-5, 5)\n",
    "\n",
    "\n",
    "res = (\n",
    "    df\n",
    "    .withColumn(\"store_product_mean\", F.mean(\"sales_quantity\").over(w1))\n",
    "    .withColumn(\"agg_level_store_mean\", F.mean(\"sales_quantity\").over(w2))\n",
    "\n",
    ")\n",
    "res.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c637b57b",
   "metadata": {},
   "source": [
    "**Avoiding the second shuffle by being explicit:**\n",
    "\n",
    "1st window partitioned by: ``product_agg_level``, ``product_id``, ``store_id``\n",
    "\n",
    "2nd window partitioned by: ``product_agg_level``, ``store_id``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cb7188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [product_id#19L, store_id#20L, day_index#21L, sales_quantity#22L, product_agg_level#23, date#24, store_product_mean#128, agg_level_store_mean#137]\n",
      "   +- Window [avg(sales_quantity#22L) windowspecdefinition(product_agg_level#23, store_id#20L, product_id#19L, day_index#21L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -3, currentrow$())) AS store_product_mean#128], [product_agg_level#23, store_id#20L, product_id#19L], [day_index#21L ASC NULLS FIRST]\n",
      "      +- Sort [product_agg_level#23 ASC NULLS FIRST, store_id#20L ASC NULLS FIRST, product_id#19L ASC NULLS FIRST, day_index#21L ASC NULLS FIRST], false, 0\n",
      "         +- Window [avg(sales_quantity#22L) windowspecdefinition(product_agg_level#23, store_id#20L, day_index#21L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -5, 5)) AS agg_level_store_mean#137], [product_agg_level#23, store_id#20L], [day_index#21L ASC NULLS FIRST]\n",
      "            +- Sort [product_agg_level#23 ASC NULLS FIRST, store_id#20L ASC NULLS FIRST, day_index#21L ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(product_agg_level#23, store_id#20L, 16), ENSURE_REQUIREMENTS, [id=#132]\n",
      "                  +- FileScan parquet [product_id#19L,store_id#20L,day_index#21L,sales_quantity#22L,product_agg_level#23,date#24] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/rocks/Untitled Folder/demo-data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:bigint,store_id:bigint,day_index:bigint,sales_quantity:bigint,product_agg_level...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "w1 = Window.partitionBy(\"product_agg_level\", \"store_id\", \"product_id\").orderBy(\"day_index\").rangeBetween(-3, 0)\n",
    "w2 = Window.partitionBy(\"product_agg_level\", \"store_id\").orderBy(\"day_index\").rangeBetween(-5, 5)\n",
    "\n",
    "\n",
    "res = (\n",
    "    df\n",
    "    .withColumn(\"store_product_mean\", F.mean(\"sales_quantity\").over(w1))\n",
    "    .withColumn(\"agg_level_store_mean\", F.mean(\"sales_quantity\").over(w2))\n",
    "\n",
    ")\n",
    "res.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd1574d",
   "metadata": {},
   "source": [
    "### Skewed data in window calculations\n",
    "\n",
    "Skewed data can be a huge source of inefficiency when using window functions. There is two reasons which should be obvious by now.\n",
    "\n",
    "**1:** Window functions cause a shuffle.\n",
    "\n",
    "**2:** Window functions do not reduce the number of rows in the dataframe.\n",
    "\n",
    "\n",
    "In ``noob``, lag feature calculation step was taking longer than we expected. This step performs window calculations over various ``partitionBy`` columns. We have discovered that the reason was just a single window, partitioned by ``product_agg_level_4``. The same step has very different run times when we remove the ``product_agg_level_4`` window:\n",
    "\n",
    "With ``product_agg_level_4``: 6 hours\n",
    "\n",
    "Without ``product_agg_level_4``: 38 mins\n",
    "\n",
    "\n",
    "Repartitioning by ``product_agg_level`` can result in a very skewed dataframe. Some groups might contain high number of products. It is good practice to avoid such window operations if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8a6a4",
   "metadata": {},
   "source": [
    "**Takeaways:**\n",
    "\n",
    "- We can avoid unnecessary shuffles when performing window calculations\n",
    "\n",
    "- Windowing over a skewed ``partitionBy`` can be extremely inefficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
