{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8979e84",
   "metadata": {},
   "source": [
    "## 1. Getting Started with Spark Internals\n",
    "\n",
    "In this series, we aim to investigate the internals of Spark. We will analyze physical plans, explore basic concepts such as ``partition`` and we'll try to improve the performance of our queries. At Invent we have been working with Spark for several years; but it seems we are still lacking a deeper understanding of it. But why? I could come up with two reasons:\n",
    "\n",
    "- Spark internals are not well documented.\n",
    "\n",
    "- Spark is written in Scala. Most of us did not write a single line of Scala.\n",
    "\n",
    "So, we often rely on third party tutorials. They become outdated quickly or do not cover the material thoroughly. This project is an attempt to fill that gap. You are encouraged to run the code in the notebooks.\n",
    "\n",
    "**Pre-requisites:**\n",
    "\n",
    "- Installing ``pyspark``\n",
    "- a working knowledge of writing queries in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9914e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window, functions as F\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.sql.shuffle.partitions\", 16).getOrCreate()\n",
    "spark.sparkContext.getConf().get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e59218",
   "metadata": {},
   "source": [
    "### Physical plans\n",
    "\n",
    "When we write a query in Spark, the result is a computation graph. Only when we call an action method, computation is performed. This lazy evaluation model is well explained in many other places. Therefore, we won't go into details here.\n",
    "\n",
    "\n",
    "We use ``DataFrame.explain`` to see the physical plan that will be used to compute the result of a query. This is basically a computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b10ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 2),\n",
    "        (1, 2),\n",
    "        (2, 4),\n",
    "        (2, 4),\n",
    "        (3, 6),\n",
    "    ],\n",
    "    [\"col1\", \"col2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba22f1",
   "metadata": {},
   "source": [
    "**Basics of physical plans:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e414e39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[sum(col1#238L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#490]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(col1#238L)])\n",
      "         +- Project [col1#238L]\n",
      "            +- Scan ExistingRDD[col1#238L,col2#239L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(F.sum(\"col1\")).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4118ef",
   "metadata": {},
   "source": [
    "\n",
    "- We read physical plans from bottom to top.\n",
    "- The step at the bottom shows how the dataframe is created. In this case, we are creating a dataframe from a collection that resides in the driver memory.\n",
    "\n",
    "See:\n",
    "\n",
    "       +- Scan ExistingRDD[col1#28L,col2#29L]\n",
    "\n",
    "The step just above the scan is ``Project``. This ``select``s just the necessary columns for our query to save memory. In this case we only need ``col1``.\n",
    "\n",
    "See:\n",
    "\n",
    "    +- Project [col1#28L]\n",
    "   \n",
    "You can ignore other steps in the physical plan for now. We will investigate them further in the following notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afba36c",
   "metadata": {},
   "source": [
    "Also note that this is not the final plan. Spark can change this plan at runtime as it has more information about the data. See:\n",
    "\n",
    "    AdaptiveSparkPlan isFinalPlan=false\n",
    "\n",
    "\n",
    "We often read parquet datasets. In that case we would see a ``Filescan`` step at the bottom of the query instead of ``Scan ExistingRDD``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b08386c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [col2#251L,col1#252] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/rocks/Untitled Folder/tmp-demo], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<col2:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.repartition(\"col1\").write.partitionBy(\"col1\").parquet(\"tmp-demo\", mode=\"overwrite\")\n",
    "df = spark.read.parquet(\"tmp-demo\")\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146730f0",
   "metadata": {},
   "source": [
    "**PartitionFilter and PushdownFilter**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbb2625",
   "metadata": {},
   "source": [
    "One optimization when reading parquet is that we can employ ``PartitionFilters``. In the above physical plan, it is empty. In the following case, we filter the dataset by ``col1``. Since the parquet is partitioned by ``col1``, ``Filescan`` now only reads the required partitions.\n",
    "\n",
    "See:\n",
    "\n",
    "         PartitionFilters: [isnotnull(col1#96), (col1#96 = 1)]\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbc7c2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [col2#95L,col1#96] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/rocks/Untitled Folder/tmp-demo], PartitionFilters: [isnotnull(col1#96), (col1#96 = 1)], PushedFilters: [], ReadSchema: struct<col2:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"tmp-demo\").filter(F.col(\"col1\") == 1).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e16971c",
   "metadata": {},
   "source": [
    "If we instead filter by ``col2``, obviously we can't have this optimization because data is not partitioned by ``col2``. But we have something similar.\n",
    "\n",
    "See:\n",
    "\n",
    "    PushedFilters: [IsNotNull(col2), EqualTo(col2,1)], ReadSchema: struct<col2:bigint>\n",
    "    \n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c496989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(col2#255L) AND (col2#255L = 1))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [col2#255L,col1#256] Batched: true, DataFilters: [isnotnull(col2#255L), (col2#255L = 1)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/rocks/Untitled Folder/tmp-demo], PartitionFilters: [], PushedFilters: [IsNotNull(col2), EqualTo(col2,1)], ReadSchema: struct<col2:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"tmp-demo\").filter(F.col(\"col2\") == 1).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39ff56e",
   "metadata": {},
   "source": [
    "The difference is that ``PartitionFilters`` allow us to skip unwanted partitions completely. In the case of ``PushedFilters``, filter is implemented by the data source itself. It queries the whole table but only the rows that satisfy the filter conditions are loaded in Spark executors. In both cases we save a lot of memory, but the ``PartitionFilter`` is faster.\n",
    "\n",
    "**Query Optimization**\n",
    "\n",
    "Even if we specify the filter later in the query, Spark understands that ``PartitionFilter`` or ``PushdownFilter`` can be employed. This is due to the ``Catalyst Optimizer`` in Spark. Spark takes our query and optimizes it as much as it can. Spark can also optimize the query further at runtime with ``Adaptive Query Execution``.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20193859",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [col2#286L, col1#287, (col1#287 * 1000) AS foo#290, (col2#286L * 1000) AS bar#294L]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [col2#286L,col1#287] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/rocks/Untitled Folder/tmp-demo], PartitionFilters: [isnotnull(col1#287), (col1#287 = 1)], PushedFilters: [], ReadSchema: struct<col2:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complicated query with a partition filter at the end.\n",
    "df = spark.read.parquet(\"tmp-demo\")\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"foo\", F.col(\"col1\") * 1000)\n",
    "    .withColumn(\"bar\", F.col(\"col2\") * 1000)\n",
    "    .filter(F.col(\"col1\") == 1)\n",
    ")\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c376df9",
   "metadata": {},
   "source": [
    "There is two stages of query optimization:\n",
    "\n",
    "**1. Catalyst Optimizer**: When we call ``explain``, we see the query optimized by Catalyst.\n",
    "\n",
    "**2. Adaptive Query Execution**: Further optimizes the query at runtime. Covered well by Databricks. See: https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1288cd92",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- Read the physical plan from bottom to top.\n",
    "\n",
    "- ``PartitionFilter`` and ``PushdownFilter`` saves us a lot memory.\n",
    "\n",
    "- Spark optimizes our query to save memory and computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
